\subsection{Models comparison}

La scelta del modello è stata dettata da tre parametri: la risoluzione delle immagini in ingresso, i tempi di esecuzione del detector per immagine e, infine, la sua capacità di individuare e classificare correttamente gli oggetti. In particolare, rispetto a quest’ultimo aspetto non ci è possibile fornire una valutazione quantitativa delle performance, in quanto davanti al robot si presenta sempre la stessa scena, pertanto abbiamo valutato i detector in base al loro comportamento relativamente a questa scena, e quindi i giudizi nella colonna “Performance” della tabella fanno riferimento a questa condizione. Inoltre, le immagini che abbiamo catturato, a valle dello stitching, hanno una dimensione di circa 900x400. Durante le nostre prove ci siamo accorti che questo tipo di immagini non sono tipici input per le reti a nostra disposizione. Alcune di queste, nonostante avessero valori di mAP di tutto rispetto sul dataset COCO (come si può vedere su \url{https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)}, non risultavano efficaci nella situazione in esame.

Inizialmente sono state provate due reti che lavorano con immagini aventi risoluzione 640x640, tali detector, facendo un downscale dell’immagine risultato del processo di stitching, hanno dimostrato pessime performance.
Abbiamo quindi deciso di provare reti che lavorano con immagini aventi una risoluzione più grande. La Faster RCNN che usa come backbone Resnet 101 e che lavora con immagini aventi una risoluzione di 1024x1204, avendo performance discrete nella detection (non tutti gli oggetti venivano rivelati ed ad altri veniva assegnata l’etichetta sbagliata), ha dimostrato che la risoluzione delle immagini date in input ai modelli deve essere un fattore discriminante nella scelta del modello.
Il passo successivo è stato, quindi, quello di provare una rete che lavorasse con immagini più simili, in risoluzione, a quelle che noi le diamo in input: la Faster RCNN che usa come backbone Resnet 50, ha dimostrato, in tempi ragionevoli, ottime performance in termini di rilevazione degli oggetti, ma a qualche oggetto veniva assegnata una label sbagliata; la Faster RCNN che usa come backbone Resnet 101 ha dimostrato le migliori performance sia per la classificazione che la detection, ma in tempi meno ragionevoli.

Nelle immagini riportanti i risultati delle detection con le varie reti, le bande verticali di colore verde fluo evidenziano la suddivisione dell’immagine utilizzata per separare tra loro le regioni sinistra, destra e centrale.

I tempi di esecuzione riportati nella tabella fanno riferimento all’esecuzione della rete all’interno della VM “di riferimento” per il corso, senza l’utilizzo di accelerazione basata su GPU. Inoltre, sono tempi “di regime”, cioè validi per tutte le esecuzioni del detector eccetto la prima, che, sperimentalmente, per motivi probabilmente legati a TensorFlow, impiega considerevolmente più tempo delle altre.

\subsection{The chosen model}

Si può notare come lo stitching permetta di fornire in input al detector una visione d’insieme della scena, che consente ad esempio di rilevare oggetti quali la scrivania, che sarebbero stati più difficili da individuare attraverso l’analisi di singole catture successive.
