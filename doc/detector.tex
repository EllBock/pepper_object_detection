\section{Detector}

\begin{comment}
    Breve descrizione e motivazione della scelta
    Tabellina che abbiamo fatto noi
    Tabellina di Object Recognition Model Zoo
    Almeno immagine finale
\end{comment}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|} 
     \hline
     \textbf{Modello} & \textbf{Input size} & \textbf{Tempo} & \textbf{Prestazioni} \\
     \hline
     SSD MobileNetV1 & $640 \times 640$ & $6$ sec & Scarse \\
     \hline
     SSD ResNet152 V1 FPN & $640 \times 640$ & $20$ sec & Molto scarse \\
     \hline
     EfficientDet D3 & $896 \times 896$ & $23$ sec & Buone \\
     \hline
     Faster-RCNN ResNet101 & $1024 \times 1024$ & $20$ sec & Buone \\
     \hline
     Faster-RCNN ResNet50 & $800 \times 1333$ & $21$ sec & Ottime \\
     \hline
     Faster-RCNN ResNet101 & $800 \times 1333$ & $30$ sec & Migliori \\
     \hline
    \end{tabular}
    \caption{Confrontro tra i modelli testati. La input size è fornita in formato $h\times w$, mentre il tempo è relativo all'inferenza su una singola immagine, eseguita su una CPU virtualizzata.}
    \label{table:modelli}
\end{table}

Nello scegliere il modello abbiamo considerato come use-case di riferimento il task assegnatoci per l'esercitazione, senza calarci in una situazione di utilizzo del robot specifica (ad esempio assistenza domiciliare). Pertanto, abbiamo ritenuto necessario dare più importanza alla precisione del risultato che ai tempi di esecuzione. In un'eventuale interazione con un utente, infatti, degli errori di detection più grossolani risalterebbero maggiormente rispetto ad un tempo di attesa prolungato. Ovviamente anche quest'ultimo ha la sua importanza, visto che non ricevere feedback per un determinato lasso di tempo potrebbe suscitare preoccupazione nell'utente. Sull'hardware fornito per il progetto ci è impossibile tener conto del tempo di esecuzione, che risulta eccessivo con la maggior parte delle reti, anche quelle poco performanti. Ciò considerato, la scelta che abbiamo fatto minimizza i tempi di esecuzione tenendo però conto della precisione necessaria affinché l'interazione risulti soddisfacente.
In particolare, rispetto a quest’ultimo aspetto non ci è possibile fornire una valutazione quantitativa delle performance, in quanto davanti al robot si presenta sempre la stessa scena, pertanto abbiamo valutato i detector in base al loro comportamento relativamente a questa scena, e quindi i giudizi nella colonna “Performance” della tabella~\ref{table:modelli} fanno riferimento soltanto a questa condizione. Inoltre, le immagini che abbiamo catturato, a valle dello stitching, hanno una dimensione di circa $900 \times 400$. Durante le nostre prove ci siamo accorti che questo tipo di immagini non sono tipici input per le reti a nostra disposizione. Infatti alcune di queste, nonostante avessero valori di mAP di tutto rispetto sul dataset COCO\footnote{\url{https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md}}, non risultavano efficaci nella situazione in esame.

Inizialmente sono state provate due reti che lavorano con immagini aventi risoluzione $640 \times 640$ che, però, facendo un downscale dell’immagine risultato del processo di stitching, hanno dimostrato pessime performance.
Abbiamo quindi deciso di provare reti che lavorano con immagini aventi una risoluzione più grande. La Faster-RCNN che usa come backbone ResNet101 con input size $1024 \times 1204$, avendo performance discrete nella detection (non tutti gli oggetti venivano rivelati ed ad altri veniva assegnata l’etichetta sbagliata), ha dimostrato che la risoluzione delle immagini in input deve essere un altro fattore discriminante nella scelta del modello.

\begin{figure}[t]
    \centering
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ssd_mobilenet}
        \caption{SSD MobileNetV1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ssd_resnet152}
        \caption{SSD ResNet101}
    \end{subfigure}
    \hfill \\
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{efficientdetd3}
        \caption{EfficientDet D3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fastercnnresnet101-1024x1024.jpg}
        \caption{Faster-RCNN ResNet101 $1024\times 1024$}
    \end{subfigure}
    \hfill \\
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fastercnnresnet50-800x1333.jpg}
        \caption{Faster-RCNN ResNet50 $800\times 1333$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fastercnnresnet101-800x1333.jpg}
        \caption{Faster-RCNN ResNet101 $800\times 1333$}
    \end{subfigure}
    \hfill
    \caption{Risultati dei modelli testati.}
\end{figure}


Il passo successivo è stato provare una rete che lavorasse con immagini più simili, in risoluzione, a quelle che noi le forniamo in input: la Faster-RCNN che usa come backbone ResNet50 ha dimostrato, in tempi simili alle altre reti, ottime performance in termini di rilevazione degli oggetti, ma a qualche oggetto veniva assegnata una label sbagliata; la Faster-RCNN che usa come backbone ResNet101 ha dimostrato le migliori performance sia per la classificazione che per la detection, ma in tempi più dilatati, pertanto abbiamo preferito tenere come nostro modello di riferimento quella basata su ResNet50.

Questo modello consiste di una \textit{Region Proposal Network} (RPN) e una rete che si occupa della classificazione, ed entrambe condividono le feature convoluzionali dell'intera immagine, consentendo così proposte regionali quasi a costo zero. Questo modello prevede quindi che le feature maps estratte dai layer convoluzionali siano dati in pasto alla RPN, la quale utilizza una sliding window sulle feature maps, e per ogni finestra genera $k$ anchor boxes di diverse forme e grandezze aventi centro nel centro della finestra \cite{resnet}. In letteratura è riscontrabile che Faster-RCNN sia una delle architetture migliori per le detection su oggetti di medie e piccole dimensioni, principalmente presenti nelle nostre immagini (vedi paragrafo~\ref{sec:master_small_img}).
