\section{Architettura}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{Architettura}
	\caption{Architettura del software}
	\label{fig:architecture}
\end{figure}

Il nostro software si compone dei seguenti tre nodi.
\begin{itemize}
    \item Un nodo che implementa il servizio di object detection, che riceve in input un’immagine da analizzare e, per ogni oggetto nell’immagine, restituisce il relativo bounding box, la classe ed il livello di confidenza relativo alla predizione.
    \item Un nodo che implementa tre servizi che forniscono un’interfaccia verso le funzionalità di text to speech, gestione della posa e movimento della testa di Pepper.
    \item Un nodo master che implementa la funzionalità richiesta dall’homework, acquisendo le immagini dal topic della camera di Pepper e utilizzando i servizi offerti dagli altri due nodi.
\end{itemize}
Per l’interfacciamento da e verso Pepper viene utilizzato il NAOqi SDK.

Abbiamo implementato le varie funzionalità come servizi ROS perché questo permette di modellarle come risorse condivise tra i vari nodi e di gestire gli accessi ad esse. L'utilizzo del paradigma request-response al posto di quello publish-subscribe permette inoltre di attendere e verificare l'esito di un'operazione, il che è importante ad esempio nella gestione dei movimenti del robot.
Non abbiamo utilizzato le azioni ROS (non bloccanti) perché per la nostra applicazione era comodo bloccare il sistema in attesa del completamento delle operazioni sul robot, e non era necessario fare altro mentre è in corso una detection.
Per quanto riguarda le funzionalità di object detection, fornirle tramite un servizio ROS rende non necessario inserire nel messaggio che trasporta il risultato della detection l'immagine di riferimento, in quanto il nodo che effettua la richiesta al servizio sa quale immagine sta fornendo in input, permettendo così di mantenere il messaggio più snello e di semplificare la sincronizzazione tra chi richiede un'operazione di detection ed il nodo che la realizza.

Vorremmo far notare che il detector restituisce il risultato al solo nodo che invia la richiesta. Questo comportamento potrebbe sembrare limitante nel caso in cui il risultato della detection su di una stessa immagine dovesse servire per più nodi, perché si potrebbe pensare di dover invocare più volte il servizio fornendo la stessa immagine in input. In realtà, questa cosa può essere evitata facendo in modo che un solo nodo invochi il servizio e pubblichi il risultato ottenuto su di un apposito topic. Questo sistema, rispetto all'utilizzo del paradigma publish-subscribe, consente di conservare la possibilità di ricevere un feedback sul risultato dell'operazione. 

Infine, per gestire i parametri di configurazione utilizziamo il parameter server di ROS, ed un file di configurazione che viene automaticamente caricato su di esso tramite il nostro launch file. 

\subsection{Server di Object Detection}


Come anticipato, uno dei tre nodi principali del sistema si occupa di esporre un servizio di object detection. 
Questo servizio è implementato attraverso una classe chiamata\linebreak\verb|PepperObjectDetectorService|, il cui diagramma UML (semplificato) è riportato in figura~\ref{fig:detector_uml}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{DetectorUml}
	\caption{Diagramma UML (semplificato) classe \texttt{PepperObjectDetectorService}}
	\label{fig:detector_uml}
\end{figure}

Nel costruttore della classe viene inizializzato il servizio ROS, al quale viene associato come handler il metodo \verb|detect_objects|, e viene inoltre creato un oggetto del tipo \verb|Detector|, che carica il modello di object detection dal percorso fornito e lo esegue sugli input che gli vengono sottomessi tramite chiamata. Il \verb|Detector| viene utilizzato in \verb|detect_objects| per eseguire la detection sugli input inviati al servizio.

Quando il nodo server viene eseguito, viene istanziato un oggetto del tipo\linebreak\verb|PepperObjectDetectorService|, ed il suo metodo \verb|stop| viene registrato per l'esecuzione al momento dello shutdown del nodo, in modo tale che il servizio venga stoppato quando l'esecuzione del server termina.
Infine il nodo entra in \verb|spin|, in modo da restare in ascolto delle richieste all'object detector.

\subsection{Server di interfaccia verso Pepper}

Il tale nodo è stato implementato per offrire le funzionalità offerte dal NAOqi Framework\footnote{Il framework usato per programmare i robot dell'\textbf{Aldebaran}, risponde alle più comuni esigenze della robotica.} senza obbligare il programmatore a scrivere codice Python 2.

Come anticipato, si occupa di esporre tre servizi: text to speech, gestione della posa e movimento della testa di Pepper. 
Questi servizi sono stati implementati attraverso una classe chiamata \verb|NaoServer|, nel cui costruttore vengono inizializzati i tre servizi ROS e viene imposta al robot la lingua inglese.

Al servizio \verb|pepper_head_mover| è stato associato come handler il metodo \verb|moveHead|, che offre un interfaccia simile a quella del metodo \verb|angleInterpolation| del proxy NAOqi chiamato \verb|ALMotion|, al quale inoltrerà la richiesta di movimento. Di fatti, il metodo del proxy NAOqi offre anche la possibilità di effettuare movimenti rispetto ad indicazioni relative, non solo assolute.

Al servizio \verb|pepper_pose| è stato associato come handler il metodo \verb|setPose|, che offre un interfaccia simile a quella del metodo \verb|goToPosture| del proxy NAOqi chiamato \verb|ALRobotPosture|, al quale inoltrerà la richiesta di movimento. Di fatti, il metodo del proxy NAOqi offre anche la possibilità di effettuare il movimento a velocità diverse, nel nostro caso abbiamo ritenuto opportuno che il robot si muovesse al $10\%$ della sua velocità massima.

Al servizio \verb|pepper_tts| è stato associato come handler il metodo \verb|say|, che offre la medesima interfaccia del metodo \verb|say| del proxy NAOqi chiamato \verb|ALTextToSpeech|, al quale inoltrerà la sua richiesta.

I tre servizi, portato a termine il lavoro richiesto, invieranno al nodo che li invoca un valore booleano, indice del successo o insuccesso dell'operazione.

Quando il nodo server viene eseguito entra in \verb|spin|, in modo da restare in attesa di ricevere richieste.

\subsection{Nodo Master}

Il nodo Master (come \emph{puppet master}, ossia il burattinaio) è stato concepito come il pezzo centrale della nostra applicazione. Questo nodo si interfaccia con tutti i nodi dell'architettura, ed esegue i passi chiave del task in maniera sequenziale. La sequenza di passi è strettamente legata al task, e risulta molto semplice implementare un nuovo nodo che lo sostituisca per personalizzare o modificare i passi, compatibilmente con le interfacce che sono a disposizione nella nostra architettura.

Abbiamo scelto l'approccio sequenziale per semplicità di implementazione. Ciò non toglie che alcune delle azioni possano ottimizzate tramite parallelismi e sincronizzazioni, ma è stato scelto di posticipare questi miglioramenti per dare precedenza al funzionamento completo del sistema secondo le specifiche.

Qui presentiamo la sequenza di operazioni che compie il nodo.
% FRASE DI INIZIO DELLE FASI
\begin{enumerate}
	\item Aspetta che tutti i servizi siano online prima di poter proseguire.
	\item Porta il robot in una posizione neutra con il servizio \verb|pepper_pose| del nodo interfaccia. 
	\item Fa muovere la testa al robot in alcune posizioni predefinite, in cui verrà ``scattata'' una fotografia. Utilizza il servizio \verb|pepper_head_mover| per passare delle traiettorie con posizione singola, visto che dovrà aspettare una immagine dal topic della camera del robot. Nell'attuale implementazione, vengono assunte dieci posizioni della testa che dividono la vista del robot in una matrice $5 \times 2$, una rotazione tra $-0.8$ rad e $0.8$ rad sull'asse $z$ (yaw), e tra $-0.2$ rad e $0.2$ rad sull'asse $y$ (pitch). Le immagini sono acquisite in modo che siano leggermente sovrapposte.
	\item Esegue lo stitching delle immagini catturate con la libreria OpenCV, ottenendo così un'immagine panoramica di tutta la scena.
	\item Richiede al servizio \verb|pepper_object_detection| gli oggetti presenti nell'immagine panoramica.
	\item Genera una frase per il robot, dividendo gli oggetti trovati tra sinistra, centro e destra in base alla posizione del centro del bounding box, che viene poi inviata al servizio \verb|pepper_tts| per essere riprodotta.
\end{enumerate}

\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{panorama}
	\caption{Vista del robot dopo l'image stitching.}\label{fig:panorama}
\end{figure}

Una scelta chiave che abbiamo preso è stata quella di eseguire uno stitching sulle immagini della vista (figura~\ref{fig:panorama}). I vantaggi di questo approccio sono molteplici: \begin{enumerate*}[label={(\arabic*)}] \item siamo in grado rilevare efficacemente oggetti che occupano uno spazio che vada oltre la singola immagine (es.\@: un tavolo); \item siamo in grado di evitare che oggetti piccoli divisi su più immagini vengano rilevati più volte, o nessuna; \item permette di allargare il campo visivo a piacere, aggiungendo più catture sia in orizzontale che in verticale.\end{enumerate*} Utilizzando altri approcci, alcune di queste proprietà sarebbero state sostanzialmente più complicate da implementare, e molto probabilmente non avrebbero avuto la stessa robustezza.

\label{sec:master_small_img}Avere una sola immagine fa sì che il detector lavori una sola volta. Se ciò sia effettivamente vantaggioso rispetto ad effettuare la detection su più immagini separate dipende da due fattori: il numero di immagini su cui fare la detection e il modello utilizzato. Nel caso di più immagini potrebbe bastare un detector leggero, visto che queste sono piccole e gli oggetti sono di meno e occupano gran parte della scena. Nel nostro caso, invece, l'immagine risulterà più grande, e soprattutto conterrà più oggetti che occuperanno, relativamente alle dimensioni, meno spazio. Come vedremo nel prossimo paragrafo, sarà necessaria una rete più precisa nel localizzare oggetti piccoli, e quindi più complessa.